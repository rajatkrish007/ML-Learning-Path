<h3>Gradient Descent</h3>

- Initialization: m_curr and b_curr (slope and intercept) are initialized to 0.

- Iterations: The loop runs for 10,000 iterations.

- Prediction: y_predicted is computed using the linear equation.

- Cost: The cost (error) is computed using Mean Squared Error.

- Gradients: The partial derivatives for m and b (gradients) are computed.

- Update Rule: The slope and intercept are updated using the gradient descent formula.

- Print Results: The values of m, b, cost, and the iteration number are printed at each step for tracking.

import numpy as np

def gradient_descent(x, y):

  # Initialize slope (m) and intercept (b) to 0
    m_curr =  b_curr = 0

  # Set the number of iterations for gradient descent
    iterations = 10000

  # Get the number of data points
    n = len(x)

  # Define the learning rate (step size)
    learning_rate = 0.001

    for i in range(iterations):

  # Calculate the predicted y values (linear equation: y = mx + b)
        y_predicted = m_curr * x + b_curr

  # Compute the cost (Mean Squared Error)
        cost = (1/n) * sum([val ** 2 for val in (y-y_predicted)])

  # Calculate the gradient for m (slope)
        md = -(2/n) * sum(x * (y - y_predicted))

  # Calculate the gradient for b (intercept)
        bd = -(2/n) * sum(y - y_predicted)

  # Update the slope and intercept using the gradients
        m_curr = m_curr - learning_rate * md
        b_curr = b_curr - learning_rate * bd

  # Print the current values of m, b, cost, and iteration count
        # print("m: ", m_curr, "b: ",  b_curr, "Cost: ", cost, "iterations: ", i)

        print("m {}, b {}, c {}, iterations {}".format(m_curr, b_curr, cost, iterations))

# Input data points for x and y
x = np.array([1,2,3,4,5])
y = np.array([5,7,9,11,13])

gradient_descent(x, y)
